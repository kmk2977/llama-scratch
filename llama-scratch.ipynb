{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "c93c016f-e29b-4517-858f-eaab0742fcef",
    "_uuid": "42c1636a-0f65-4dcc-b12f-ad22edc72f4a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "4082f356-43f2-4dc6-be38-0b22995915df",
    "_uuid": "78889705-8052-4735-9c98-927a393be662",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 32\n",
    "    n_heads: int = 32\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = -1 # Later set in the build method\n",
    "    multiple_of: int = 256\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    # Needed for KV cache\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "\n",
    "    device: str = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "262f62c6-bef8-451c-9e71-e095e714a9f2",
    "_uuid": "648cbadb-f154-494e-aa83-c59b4a525d8e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def theta_pos_freq(head_dim: int, seq_len: int, device: str, theta: float = 10000.0):\n",
    "    theta_numerator = torch.arange(0, head_dim, 2).float()\n",
    "    theta = 1.0 / (theta ** (theta_numerator / head_dim)).to(device)\n",
    "    m = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(m, theta).float()\n",
    "    freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "7ccca915-ecb9-4068-a6c7-67d1790e2990",
    "_uuid": "37ddfd07-0241-46d0-937e-3824fa20f450",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def rotary_embeddings(x: torch.Tensor, freqs_complex: torch.Tensor, device: str):\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
    "    x_rotated = x_complex * freqs_complex\n",
    "    x_out = torch.view_as_real(x_rotated)\n",
    "    x_out = x_out.reshape(*x.shape)\n",
    "    return x_out.type_as(x).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "0eaa26a6-a1bd-44a4-9f8b-2c023e36b9a0",
    "_uuid": "a0e626de-a070-4be5-afbe-1da3fe003127",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    batch_size, seq_len, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    else:\n",
    "        return(\n",
    "            x[:, :, :, None, :]\n",
    "            .expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim)\n",
    "            .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "ff79728a-6d50-4b27-805a-0755b9ac0b40",
    "_uuid": "a540ab31-862f-46e1-ac62-ce74ccfeb7b4",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        # The gamma parameter\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x: torch.Tensor):\n",
    "        # (B, Seq_Len, Dim) * (B, Seq_Len, 1) = (B, Seq_Len, Dim)\n",
    "        # rsqrt: 1 / sqrt(x)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # (Dim) * (B, Seq_Len, Dim) = (B, Seq_Len, Dim)\n",
    "        return self.weight * self._norm(x.float()).type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "014361a9-6ae5-44a8-8b7c-7d3f4fe58489",
    "_uuid": "a1165e2e-a37b-4dda-872d-83bbdf44d92b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Indicates the number of heads for the Keys and Values\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        # Indicates the number of heads for the Queries\n",
    "        self.n_heads_q = args.n_heads\n",
    "        # Indicates how many times the Keys and Values should be repeated\n",
    "        self.n_rep = self.n_heads_q // self.n_kv_heads\n",
    "        # Indicates the dimension of each head, that is, the part of the embedding that each head will be responsible for\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "\n",
    "        self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
    "        self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_complex: torch.Tensor\n",
    "    ):\n",
    "        batch_size, seq_len, _ = x.shape  # (B, 1, Dim)\n",
    "\n",
    "        # (B, 1, Dim) -> (B, 1, H_Q * Head_Dim)\n",
    "        xq = self.wq(x)\n",
    "        # (B, 1, Dim) -> (B, 1, H_KV * Head_Dim)\n",
    "        xk = self.wk(x)\n",
    "        # (B, 1, Dim) -> (B, 1, H_KV * Head_Dim)\n",
    "        xv = self.wv(x)\n",
    "\n",
    "        # (B, 1, H_Q * Head_Dim) -> (B, 1, H_Q, Head_Dim)\n",
    "        xq = xq.view(batch_size, seq_len, self.n_heads_q, self.head_dim)\n",
    "        # (B, 1, H_KV * Head_Dim) -> (B, 1, H_KV, Head_Dim)\n",
    "        xk = xk.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        # (B, 1, H_KV * Head_Dim) -> (B, 1, H_KV, Head_Dim)\n",
    "        xv = xv.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        # (B, 1, H_Q, Head_Dim) --> (B, 1, H_Q, Head_Dim)\n",
    "        xq = rotary_embeddings(xq, freqs_complex, device=x.device)\n",
    "        # (B, 1, H_KV, Head_Dim) --> (B, 1, H_KV, Head_Dim)\n",
    "        xk = rotary_embeddings(xk, freqs_complex, device=x.device)\n",
    "\n",
    "        # Replace the entry in the cache\n",
    "        self.cache_k[:batch_size, start_pos : start_pos + seq_len] = xk\n",
    "        self.cache_v[:batch_size, start_pos : start_pos + seq_len] = xv\n",
    "\n",
    "        # (B, Seq_Len_KV, H_KV, Head_Dim)\n",
    "        keys = self.cache_k[:batch_size, : start_pos + seq_len]\n",
    "        # (B, Seq_Len_KV, H_KV, Head_Dim)\n",
    "        values = self.cache_v[:batch_size, : start_pos + seq_len]\n",
    "\n",
    "        # Since every group of Q shares the same K and V heads, just repeat the K and V heads for every Q in the same group.\n",
    "\n",
    "        # (B, Seq_Len_KV, H_KV, Head_Dim) --> (B, Seq_Len_KV, H_Q, Head_Dim)\n",
    "        keys = repeat_kv(keys, self.n_rep)\n",
    "        # (B, Seq_Len_KV, H_KV, Head_Dim) --> (B, Seq_Len_KV, H_Q, Head_Dim)\n",
    "        values = repeat_kv(values, self.n_rep)\n",
    "\n",
    "        # (B, 1, H_Q, Head_Dim) -> (B, H_Q, 1, Head_Dim)\n",
    "        xq = xq.transpose(1, 2)\n",
    "        # (B, Seq_Len_KV, H_Q, Head_Dim) -> (B, H_Q, Seq_Len_KV, Head_Dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        # (B, Seq_Len_KV, H_Q, Head_Dim) -> (B, H_Q, Seq_Len_KV, Head_Dim)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # (B, H_Q, 1, Head_Dim) @ (B, H_Q, Head_Dim, Seq_Len_KV) -> (B, H_Q, 1, Seq_Len_KV)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        # (B, H_Q, 1, Seq_Len_KV) -> (B, H_Q, 1, Seq_Len_KV)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "\n",
    "        # (B, H_Q, 1, Seq_Len) @ (B, H_Q, Seq_Len_KV, Head_Dim) -> (B, H_Q, 1, Head_Dim)\n",
    "        output = torch.matmul(scores, values)\n",
    "        # (B, H_Q, 1, Head_Dim) -> (B, 1, H_Q, Head_Dim) -> (B, 1, Dim)\n",
    "        output = (output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1))\n",
    "        return self.wo(output) # (B, 1, Dim) -> (B, 1, Dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "0eb0a38c-089a-4659-bcb7-a860a6f6164a",
    "_uuid": "02c113a1-711c-4b1d-b963-a143def19ebe",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        hidden_dim = 4 * args.dim\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        if args.ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(args.ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = args.multiple_of * ((hidden_dim + args.multiple_of - 1) // args.multiple_of)\n",
    "        \n",
    "        self.w1 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, args.dim, bias=False)\n",
    "        self.w3 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        swish = F.silu(self.w1(x))\n",
    "        x_V = self.w3(x)\n",
    "        x = swish * x_V\n",
    "        x = self.w2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b29c7f66-033d-4335-9bdb-56ae5cbb3461",
    "_uuid": "e3a82d37-c44f-4f0c-a456-99bf5c8e660b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        \n",
    "        self.attention = SelfAttention(args)\n",
    "        self.feed_forward = FeedForward(args)\n",
    "        \n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "    \n",
    "    def forward(self, x:torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
    "        h = x + self.attention.forward(self.attention_norm(x), start_pos, freqs_complex)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "5a8bc330-4391-4c10-8772-5457deb967e4",
    "_uuid": "8cd279fa-2c1f-4a5d-b5f9-278f8689efc7",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        assert args.vocab_size != -1, \"Vocab size must be set\"\n",
    "\n",
    "        self.args = args\n",
    "        self.vocab_size = args.vocab_size\n",
    "        self.n_layers = args.n_layers\n",
    "        self.tok_embeddings = nn.Embedding(self.vocab_size, args.dim)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for layer_id in range(args.n_layers):\n",
    "            self.layers.append(EncoderBlock(args))\n",
    "\n",
    "        self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.output = nn.Linear(args.dim, self.vocab_size, bias=False)\n",
    "\n",
    "        self.freqs_complex = theta_pos_freq(self.args.dim // self.args.n_heads, self.args.max_seq_len * 2, device=self.args.device)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        # (B, Seq_Len)\n",
    "        batch_size, seq_len = tokens.shape\n",
    "        assert seq_len == 1, \"Only one token at a time can be processed\"\n",
    "\n",
    "        # (B, Seq_Len) -> (B, Seq_Len, Dim)\n",
    "        h = self.tok_embeddings(tokens)\n",
    "\n",
    "        # Retrieve the pairs (m, theta) corresponding to the positions [start_pos, start_pos + seq_len]\n",
    "        freqs_complex = self.freqs_complex[start_pos:start_pos + seq_len]\n",
    "        \n",
    "        # Consecutively apply all the encoder layers\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_complex)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h).float()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "22b2d426-e90a-4888-ab7a-6d02530b72c3",
    "_uuid": "cd80711a-09bf-4dc6-b477-399f0c92aea4"
   },
   "source": [
    "# ***Inference***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "c1ab3a40-d71a-4de0-917e-f91d4d984885",
    "_uuid": "a1cd0416-ee4e-4782-980d-ea265b07b92d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Loading checkpoint \"llama-2-7b\\consolidated.00.pth\"\n",
      "Loaded checkpoint in 26.06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vaibhavi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\__init__.py:749: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\\torch\\csrc\\tensor\\python_tensor.cpp:433.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded state dict in 161.70s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tokens: 100%|███████████████████████████████████████████████████████████| 114/114 [1:59:10<00:00, 62.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simply put, the theory of relativity states that 1) time is relative to the observer, 2) mass is relative to the observer, 3) speed is relative to the observer, and 4) energy is relative to the observer.–\n",
      "The theory of relativity is a set of two theories, the first being special relativity, which explains the relationship between mass and energy. The second theory is general relativity, which explains the relationship between mass and gravity.\n",
      "The theory of relativity was developed by Albert Einstein in 1\n",
      "--------------------------------------------------\n",
      "If Google was an Italian company founded in Milan, it would be listed on the Milan Stock Exchange, as the American giant is registered in the city. MasterCard has decided to set up its headquarters in Italy. The US company, which is listed on the New York Stock Exchange, has announced the acquisition of a majority stake in the Milan-based financial company Nexi, which controls the Italian Post Office. The deal is worth 21 billion euros.\n",
      "Nexi is the main player in the Italian payment sector, with 20 million\n",
      "--------------------------------------------------\n",
      "Translate English to French:\n",
      "        \n",
      "        sea otter => loutre de mer\n",
      "        peppermint => menthe poivrée\n",
      "        plush girafe => girafe peluche\n",
      "        cheese => fromage\n",
      "        onion => oignon\n",
      "        cherry => cerise\n",
      "        raspberry => framboise\n",
      "        coffee => café\n",
      "        tea => thé\n",
      "        cat => chat\n",
      "        dog => chien\n",
      "        horse => cheval\n",
      "        pig => porc\n",
      "        sheep => m\n",
      "--------------------------------------------------\n",
      "Tell me soemthing about the following person is he actually an alien disguised as human:\n",
      "        Name: Elon Musk\n",
      "        Decision: \n",
      "        \n",
      "        I believe he is an alien. He is an alien disguised as human. He is the CEO of Tesla, the founder of SpaceX, and the founder of The Boring Company. He is the richest person in the world. He is the most innovative person in the world. He is the most genius person in the world.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class LLaMA:\n",
    "\n",
    "    def __init__(self, model: Transformer, tokenizer: SentencePieceProcessor, model_args: ModelArgs):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.args = model_args\n",
    "\n",
    "    @staticmethod\n",
    "    def build(checkpoints_dir: str, tokenizer_path: str, load_model: bool, max_seq_len: int, max_batch_size: int, device: str):\n",
    "        prev_time = time.time()\n",
    "        if load_model:\n",
    "            checkpoints = sorted(Path(checkpoints_dir).glob(\"*.pth\"))\n",
    "            assert len(checkpoints) > 0, f\"no checkpoint files found in {checkpoints_dir}\"\n",
    "            ckpt_path = checkpoints[0]\n",
    "            print(f'Loading checkpoint \"{ckpt_path}\"')\n",
    "            checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "            print(f\"Loaded checkpoint in {time.time() - prev_time:.2f}s\")\n",
    "            prev_time = time.time()\n",
    "        with open(Path(checkpoints_dir) / \"params.json\", \"r\") as f:\n",
    "            params = json.loads(f.read())\n",
    "\n",
    "        model_args: ModelArgs = ModelArgs(\n",
    "            max_seq_len=max_seq_len,\n",
    "            max_batch_size=max_batch_size,\n",
    "            device=device,\n",
    "            **params\n",
    "        )\n",
    "\n",
    "        tokenizer = SentencePieceProcessor()\n",
    "        tokenizer.load(tokenizer_path)\n",
    "        model_args.vocab_size = tokenizer.vocab_size()\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "        else:\n",
    "            torch.set_default_tensor_type(torch.BFloat16Tensor)\n",
    "        \n",
    "        model = Transformer(model_args).to(device)\n",
    "\n",
    "        if load_model:\n",
    "            # The only unmatched key in the checkpoint is rope.freqs. Remove it\n",
    "            del checkpoint['rope.freqs']\n",
    "            model.load_state_dict(checkpoint, strict=True)\n",
    "            print(f\"Loaded state dict in {time.time() - prev_time:.2f}s\")\n",
    "        \n",
    "        return LLaMA(model, tokenizer, model_args)\n",
    "\n",
    "    def text_completion(self, prompts: list[str], temperature: float = 0.6, top_p: float = 0.9, max_gen_len: Optional[int] = None):\n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = self.args.max_seq_len - 1\n",
    "        # Convert each prompt into tokens\n",
    "        prompt_tokens = [self.tokenizer.encode(prompt, out_type=int, add_bos=True, add_eos=False) for prompt in prompts]\n",
    "        # Make sure the batch size is not too large\n",
    "        batch_size = len(prompt_tokens)\n",
    "        assert batch_size <= self.args.max_batch_size, f\"batch size must be less than or equal to {self.args.max_batch_size}\"\n",
    "        max_prompt_len = max(len(prompt) for prompt in prompt_tokens)\n",
    "        # Make sure the prompt length is not larger than the maximum sequence length\n",
    "        assert max_prompt_len <= self.args.max_seq_len, f\"prompt length must be less than or equal to {self.args.max_seq_len}\"\n",
    "        total_len = min(self.args.max_seq_len, max_gen_len + max_prompt_len)\n",
    "\n",
    "        # Create the list that will contain the generated tokens, along with the initial prompt tokens\n",
    "        pad_id = self.tokenizer.pad_id()\n",
    "        tokens = torch.full((batch_size, total_len), pad_id, dtype=torch.long, device=device)\n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            # Populate the initial tokens with the prompt tokens\n",
    "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=device)\n",
    "        \n",
    "        eos_reached = torch.tensor([False] * batch_size, device=device)\n",
    "        prompt_tokens_mask = tokens != pad_id # True if the token is a prompt token, False otherwise\n",
    "        cur_iterator = tqdm(range(1, total_len), desc=\"Generating tokens\")\n",
    "        for cur_pos in cur_iterator:\n",
    "            with torch.no_grad():\n",
    "                logits = self.model.forward(tokens[:, cur_pos-1:cur_pos], cur_pos)\n",
    "            if temperature > 0:\n",
    "                # The temperature is applied before the softmax\n",
    "                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "                next_token = self._sample_top_p(probs, top_p)\n",
    "            else:\n",
    "                # Greedily select the token with the max probability\n",
    "                next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "\n",
    "            next_token = next_token.reshape(-1)\n",
    "            # Only replace token if it is a padding token\n",
    "            next_token = torch.where(prompt_tokens_mask[:, cur_pos], tokens[:, cur_pos], next_token)\n",
    "            tokens[:, cur_pos] = next_token\n",
    "            # EOS is reached only if we found an EOS token for a padding position\n",
    "            eos_reached |= (~prompt_tokens_mask[:, cur_pos]) & (next_token == self.tokenizer.eos_id)\n",
    "            if all(eos_reached):\n",
    "                break\n",
    "\n",
    "        out_tokens = []\n",
    "        out_text = []\n",
    "        for prompt_index, current_prompt_tokens in enumerate(tokens.tolist()):\n",
    "            # Cut to the EOS token, if present\n",
    "            if self.tokenizer.eos_id in current_prompt_tokens:\n",
    "                eos_idx = current_prompt_tokens.index(self.tokenizer.eos_id)\n",
    "                current_prompt_tokens = current_prompt_tokens[:eos_idx]\n",
    "            out_tokens.append(current_prompt_tokens)\n",
    "            out_text.append(self.tokenizer.decode(current_prompt_tokens))\n",
    "        return (out_tokens, out_text)\n",
    "    \n",
    "    def _sample_top_p(self, probs, p):\n",
    "        # (B, vocab_size)\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "        # (B, vocab_size)\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "        # (B, vocab_size)\n",
    "        # (Substracting \"probs_sort\" shifts the cumulative sum by 1 position to the right before masking)\n",
    "        mask = probs_sum - probs_sort > p \n",
    "        # Zero out all the probabilities of tokens that are not selected by the Top P\n",
    "        probs_sort[mask] = 0.0 \n",
    "        # Redistribute the probabilities so that they sum up to 1.\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "        # Sample a token (its index) from the top p distribution\n",
    "        next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "        # Get the token position in the vocabulary corresponding to the sampled index\n",
    "        next_token = torch.gather(probs_idx, -1, next_token) \n",
    "        return next_token\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    allow_cuda = False\n",
    "    device = 'cuda' if torch.cuda.is_available() and allow_cuda else 'cpu'\n",
    "    print(device)\n",
    "\n",
    "    prompts = [\n",
    "        \"Simply put, the theory of relativity states that \",\n",
    "        \"If Google was an Italian company founded in Milan, it would\",\n",
    "        # Few shot promt\n",
    "        \"\"\"Translate English to French:\n",
    "        \n",
    "        sea otter => loutre de mer\n",
    "        peppermint => menthe poivrée\n",
    "        plush girafe => girafe peluche\n",
    "        cheese =>\"\"\",\n",
    "        # Zero shot prompt\n",
    "        \"\"\"Tell me soemthing about the following person is he actually an alien disguised as human:\n",
    "        Name: Elon Musk\n",
    "        Decision: \n",
    "        \"\"\"\n",
    "    ]\n",
    "\n",
    "    model = LLaMA.build(\n",
    "        checkpoints_dir='llama-2-7b/',\n",
    "        tokenizer_path='tokenizer.model',\n",
    "        load_model=True,\n",
    "        max_seq_len=1024,\n",
    "        max_batch_size=len(prompts),\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    out_tokens, out_texts = (model.text_completion(prompts, max_gen_len=64))\n",
    "    assert len(out_texts) == len(prompts)\n",
    "    for i in range(len(out_texts)):\n",
    "        print(f'{out_texts[i]}')\n",
    "        print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3a0a3076-3322-483c-9899-9333d96cd86f",
    "_uuid": "b6d942bf-4bb8-4e02-bd0a-67a6c8742444",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
